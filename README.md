# pdf_parser_project
Scripts for downloading and formatting papers from arXiv and PLOS to train Nougat models

Nougat includes code for training models if you can create your own dataset. Datasets must consist of two folders, one containing PDF files and another containing HTML files with corresponding filenames.

The project consisted of 3 parts:
  1. Write a script that could generate an arXiv dataset (as the Nougat team did)
  2. Figure out another source of data that was not used in the Nougat paper
  3. Show that the Nougat training code could use the data to train a new model

## arXiv scraper

`arxiv_downloader.py` (within the `arxiv` folder) attempts to download some number of the most recently uploaded papers to arXiv. It downloads both the PDF and the source as a `.tar.gz`. The script unzips the source and runs `latexml` on the source to generate HTML files. It then deletes all of the unnecessary folders and log files, leaving you with one folder `pdf` and one folder `html`. Not all `.tex` files can be successfully converted to HTML, so you will end up with fewer PDFs and HTMLs than you initially requested (you usually get around 75-80% successful conversion). With more time, I would modify the script to continue downloading more papers until you hit the user's requested number. `latexml` is very slow, so to speed up the script I used multiprocessing.

To run this script, specify the number of papers you wish to download:

```
python arxiv_downloader.py 100
```

## PLOS scraper

I tried a bunch of random ideas for finding a new source of data. I struggled to find large datasets of Word documents, as most industries that use them just upload the PDF generated by them instead of the Word document itself. I also investigated using GPT-4 to parse PDFs, but it did a very poor job, frequently skipping text randomly.

I decided to look for other sources of scientific papers, but wanted one that would have substantially different content and formatting than arXiv, which generally features quantitative subjects and is generated with LaTeX. Most other repositories of papers wouldn't let you download both PDF and text, such as PubMed Central. I eventually found PLOS, an open access journal which posts both PDFs and XML files, has an API, and mainly has biology papers written in Word. 

I wrote a second scraper which downloaded PDFs and XML files, and used Pandoc to conver the XML to HTML. As before, not all papers would successfully download or convert, and I got roughly the same conversion rate as with arXiv (75-80%).

`plos_scraper.py` is in the `plos` folder. As before, specify the number of papers you wish to download:
```
python plos_downloader.py 100
```

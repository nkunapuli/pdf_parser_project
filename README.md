# pdf_parser_project
Scripts for downloading and formatting papers from arXiv and PLOS to train Nougat models

Nougat includes code for training models if you can create your own dataset. Datasets must consist of two folders, one containing PDF files and another containing HTML files with corresponding filenames.

The project consisted of 3 parts:
  1. Write a script that could generate an arXiv dataset (as the Nougat team did)
  2. Figure out another source of data that was not used in the Nougat paper
  3. Show that the Nougat training code could use the data to train a new model

## arXiv scraper

`arxiv_downloader.py` attempts to download some number of the most recently uploaded papers to arXiv. It downloads both the PDF and the source as a `.tar.gz`. The script unzips the source and runs `latexml` on the source to generate HTML files. It then deletes all of the unnecessary folders and log files, leaving you with one folder `pdf` and one folder `html`. Not all `.tex` files can be successfully converted to HTML, so you will end up with fewer PDFs and HTMLs than you initially requested (you usually get around 75-80% successful conversion). With more time, I would modify the script to continue downloading more papers until you hit the user's requested number. `latexml` is very slow, so to speed up the script I used multiprocessing.

To run this script, specify the number of papers you wish to download:

```
python arxiv_downloader.py 100
```

## PLOS scraper

I tried a bunch of random ideas for finding a new source of data. I struggled to find large datasets of Word documents, as most industries that use them just upload the PDF generated by them instead of the Word document itself. I also investigated using GPT-4 to parse PDFs, but it did a very poor job, frequently skipping text randomly.

I decided to look for other sources of scientific papers, but wanted one that would have substantially different content and formatting than arXiv, which generally features quantitative subjects and is generated with LaTeX. Most other repositories of papers wouldn't let you download both PDF and text, such as PubMed Central. I eventually found PLOS, an open access journal which posts both PDFs and XML files, has an API, and mainly has biology papers written in Word. 

I wrote a second scraper which downloaded PDFs and XML files, and used Pandoc to conver the XML to HTML. As before, not all papers would successfully download or convert, and I got roughly the same conversion rate as with arXiv (75-80%).

As before, specify the number of papers you wish to download:
```
python plos_downloader.py 100
```

## Training with Nougat
Unfortunately I did not have time to finish this part, as it was a lot more complicated than I expected.

The first parts were successful. First, you have to run one of their scripts which converts the HTML and PDF files into Markdown and PNG files which the model actually trains on. This step worked, although many of the conversions failed. From 80 papers, maybe only 30-35 would successfully convert to Markdown and PNG. This suggests that for however many papers you wish to train on, you must initially download at least twice as many, and probably even more.

However, I was unable to start actual model training because the setup took a very long time. Their training script requires a configuration YAML file, but they do not specify which parameters are necessary and what values they should take. Some parameters are explicitly given in the paper (such as `input_size`), but others have to be guessed at (such as `encoder_layer`), or are referenced from other papers (such as the tokenizer, which points to the Galactica paper and itself does not describe the tokenizer in any detail). I got a lot of the parameters figured out, and was in the process of training a byte-pair encoding tokenizer on my training data, but ultimately time ran out before I could try to train the model. 
